# Terms
## 1. 신경망 복습
### [신경망의 학습]
- 활성화 함수
    - '비선형' 효과를 부여함으로써 신경망의 표현력을 높일 수 있음
        - Sigmoid : 'S'자 모양의 곡선 함수로 0에서 1사이의 실수 출력
        - Softmax : 0.0 이상 1.0 이하의 실수. 원소들을 모두 더하면 1.0이 되기 때문에 소프트맥스의 출력을 '확률'로 해석할 수 있음
- 손실 함수
    - 신경망의 학습에는 학습이 얼마나 잘되고 있는지 알기 위한 '척도'가 필요한데, 그 척도로 손실을 사용함. 
    - 신경망의 손실은 손실 함수를 사용해 구함. multi-class classification 신경망에서는 손실 함수로 주로 **Cross Entropy Error**를 사용. 
- 가중치 갱신
    - 1단계 : 미니배치 - 훈련 데이터 중에서 무작위로 다수의 데이터를 골라냄
    - 2단계 : 기울기 계산 - 오차역전파법으로 각 가중치 매개변수에 대한 손실함수의 기울기를 구함 (SGD를 이용)
    - 3단계 : 매개변수 갱신 - 기울기를 사용하여 가중치 매개변수를 갱신
    - 4단계 : 반복 - 1~3단계를 필요한만큼 반복

## 2. 자연어처리의 역사
### [피드포워드 신경망(흐름이 단방향인 신경망)]
- 시소러스를 사용한 기법(WordNet<유의어 사전>)
    -  NLTK 모듈
    - 단점
        - 시대 변화에 대응하기 어렵다.
        - 사람을 쓰는 비용이 크다.
        - 단어의 미묘한 차이를 표현할 수 없다.
- 통계 기반 기법
    - 단어의 분산 표현 사용 : 단어를 고정 길이의 밀집벡터로 표현. 밀집벡터란 원소가 0이 아닌 실수임. 
    - 분포 가설 : 단어의 의미는 주변 단어에 의해 형성된다는 것. 단어 자체에는 의미가 없고, 그 단어가 사용된 맥락이 의미를 형성한다는 것. 
    - 위의 두가지를 가지고 동시발생 행렬을 만들고, PPMI 행렬로 변환한 다음, 안전성을 높이기 위해 SVD를 이용해 차원을 감소시켜 각 단어의 분산 표현을 만들어냄. (밀집벡터로 변환)
    - 결과적으로 단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것!
    - 단점
        - 대규모 말뭉치를 다룰 때는 SVD 계산에서 행렬의 크기가 너무 커지기 때문에 구리다.
- 추론 기반 기법(word2vec)
    - 미니배치로 학습함. 말뭉치의 어휘 수가 많아도 처리 가능임.
    - word2vec은 단순한 2층 신경망(MatMul계층과 Softmax with Loss계층)이고, skip-gram 모델(하나의 타깃으로부터 맥락을 추측함)과 CBOW 모델(여러 맥락으로부터 하나의 타깃을 추측)을 제공함
    - word2vec은 가중치를 다시 학습할 수 있으므로, 단어의 분산표현 갱신이나 새로운 단어를 효율적으로 수행할 수 있음(통계 기반에서는 추가되면 처음부터 다시 계산해야함..)
    - 이 이후에 추론 기반 기법과 통계 기반 기법을 융합한 GloVe 기법이 등장!
        - 말뭉치 전체의 통계 정보를 손실 함수에 도입해 미니배치 학습하는 거
    - 단점
        - 말뭉치에 포함된 어휘 수가 많아지면 계산량도 커짐 = 시간 이빠이 걸림.
        - 원핫 표현으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커져서 메모리를 이빠이 차지함. > 사실 행렬의 특정 행을 추출하는거잖아..!
        - Softmax 계층에서도 동일하게 계산량이 이빠이 많음..
- 속도가 개선된 word2vec
    - Embedding 계층 사용 : 단어의 분산 표현을 담고 있으며, 지정한 단어 ID의 벡터를 추출
        - 단어의 의미가 녹아들어 있어, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치함. 
        - 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있음
    - 네거티브 샘플링 : 부정적 예를 몇 개 샘플링하는 기법. 다중 분류를 이진 분류처럼 취급할 수 있음 (해당 blank에 'you'가 정답이냐? yes/no)
### [순환 신경망]
- 피드포워드 신경망의 단점 : 시계열 데이터를 잘 다루지 못함. 
- RNN
    - RNN은 순환하는 경로가 있고, 이를 통해 내부에 '은닉 상태'를 기억할 수 있음
    - 다수의 RNN 계층이 연결된 신경망으로 해석할 수 있으며, 보통의 오차역전파법으로 학습할 수 있음 (=BPTT)
    - 긴 시계열 데이터를 학습할 때는 데이터를 적당한 길이씩 모으고('블록'이라고 함) 블록 단위로 BPTT에 의한 학습을 수행 (=Truncated BPTT)
        - 역전파의 연결을 '끊는다'
        - 순전파의 연결을 유지하기 위해 데이터를 '순차적'으로 입력해야 함
    - 언어 모델은 단어 시퀀스를 확률로 해석함
    - RNN 계층을 이용한 조건부 언어 모델은 등장한 모든 단어의 정보를 기억할 수 있음
    - 단점
        - 시계열 데이터에서 시간적으로 멀리 떨어진, 장기 의존 관계를 잘 학습할 수 없음 >BPTT에서 기울기 소실 혹은 기울기 폭발이 일어나기 때문..

- 게이트가 추가된 RNN(LSTM,GRU)
    - 기울기 폭발에는 기울기 클리핑, 기울기 소실에는 LSTM과 GRU 등이 효과적임
    - LSTM에는 input 게이트, forget 게이트, output 게이트 등 3개의 게이트가 있음
    - 게이트에는 전용 가중치가 있으며, 시그모이드 함수를 사용해서 0.0~1.0 사이의 실수를 출력함
    - 언어 모델 개선에는 LSTM 계층 다양화, 드롭아웃, 가중치 공유 등의 기법이 효과적임
- RNN을 사용한 문장 생성(seq2seq)
    - RNN을 이용한 언어모델은 새로운 문장을 생성할 수 있음
    - 문장을 생성할 때는 하나의 단어를 주고 모델의 출력(확률분포)에서 샘플링하는 과정을 반복함
    - seq2seq
        - Encoder가 출발어 입력문을 인코딩하고, 인코딩된 정보를 Decoder가 받아 디코딩하여 도착어 출력문을 얻음
        - 입력문을 반전시키는 기법(Reverse),인코딩된 정보를 Decoder의 여러 계층에 전달하는 기법(Peeky)는 seq2seq의 정확도 향상에 효과적임
        - 기계 번역, 챗봇, 이미지 캡셔닝 등 다양한 곳에 쓰임
        - 단점
            - Encoder의 출력은 '고정 길이의 벡터'여서, 엄청나게 긴 문장도 정해진 벡터 길이에 밀어 넣은 것임.
- 어텐션
    - 두 시계열 데이터 사이의 대응 관계를 데이터로부터 학습함
    - 어텐션에서는 벡터의 내적을 사용해서 벡터 사이의 유사도를 구하고, 그 유사도를 이용한 가중합 벡터가 어텐션의 출력이 됨
    - 어텐션에서 사용하는 연산은 미분 가능하기 때문에 오차역전파법으로 학습 가능
    - 어텐션이 산출하는 가중치(확률)를 시각화하면 입출력의 대응 관계를 볼 수 있음
